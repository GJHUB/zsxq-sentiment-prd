"""çŸ¥è¯†æ˜Ÿçƒè‚¡ç¥¨èˆ†æƒ…åˆ†æå™¨ - ä¸»å…¥å£

ç”¨æ³•ï¼š
  python main.py fetch --start-date 2026-02-14          # çˆ¬å–æ•°æ®ï¼ˆæ‰€æœ‰æ˜Ÿçƒï¼‰
  python main.py fetch                                   # å¢é‡çˆ¬å–ï¼ˆä»ä¸Šæ¬¡ä½ç½®ç»§ç»­ï¼‰
  python main.py analyze --data data/topics_xxx.json     # åˆ†ææ•°æ®
  python main.py run --start-date 2026-02-14             # çˆ¬å–+åˆ†æä¸€æ¡é¾™
  python main.py run                                     # å¢é‡çˆ¬å–+åˆ†æ
"""

import argparse
import asyncio
import json
import logging
import sys
from datetime import datetime
from pathlib import Path

from src.auth import AuthManager
from src.config import get_config
from src.crawler import ZsxqCrawler
from src.analyzer import SentimentAnalyzer
from src.report import ReportGenerator
from src.notify import WeChatNotifier


def setup_logging():
    """é…ç½®æ—¥å¿—"""
    log_dir = get_config("log_dir", "logs")
    Path(log_dir).mkdir(parents=True, exist_ok=True)

    today = datetime.now().strftime("%Y-%m-%d")
    log_file = Path(log_dir) / f"{today}.log"

    logging.basicConfig(
        level=getattr(logging, get_config("log_level", "INFO")),
        format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
        handlers=[
            logging.FileHandler(log_file, encoding="utf-8"),
            logging.StreamHandler(sys.stdout),
        ],
    )


async def do_fetch(args) -> str:
    """çˆ¬å–æ‰€æœ‰æ˜Ÿçƒçš„å¸–å­å’Œè¯„è®ºï¼Œä¿å­˜åˆ°JSON"""
    logger = logging.getLogger(__name__)
    notifier = WeChatNotifier()
    auth = AuthManager(
        cookie_path=get_config("cookie_path"),
        notify_func=notifier.send_image,
    )

    # è·å–Cookie
    cookie = await auth.get_cookie()
    if not cookie:
        notifier.send_alert("Cookieè·å–å¤±è´¥ï¼Œè¯·æ£€æŸ¥")
        logger.error("Cookieè·å–å¤±è´¥ï¼Œé€€å‡º")
        return ""

    group_ids = get_config("group_ids", [])
    if not group_ids:
        logger.error("æœªé…ç½® ZSXQ_GROUP_ID")
        return ""

    all_topics = []
    group_names = {}  # {group_id: group_name}
    end_date = getattr(args, "end_date", None) or datetime.now().strftime("%Y-%m-%d")

    for gid in group_ids:
        logger.info("=== çˆ¬å–æ˜Ÿçƒ: %s ===", gid)
        crawler = ZsxqCrawler(group_id=gid, cookie=cookie)

        # è·å–æ˜Ÿçƒåç§°
        try:
            import requests as req
            cookie_str = "; ".join(f"{k}={v}" for k, v in cookie.items())
            resp = req.get(
                f"https://api.zsxq.com/v2/groups/{gid}",
                headers={"Cookie": cookie_str, "User-Agent": "Mozilla/5.0",
                         "Origin": "https://wx.zsxq.com", "Referer": "https://wx.zsxq.com/"},
                timeout=10,
            )
            gdata = resp.json()
            if gdata.get("succeeded"):
                gname = gdata["resp_data"]["group"].get("name", gid)
                group_names[gid] = gname
                logger.info("æ˜Ÿçƒåç§°: %s", gname)
        except Exception:
            group_names[gid] = gid

        # ç¡®å®šèµ·å§‹æ—¶é—´ï¼šä¼˜å…ˆå‘½ä»¤è¡Œå‚æ•°ï¼Œå…¶æ¬¡ä¸Šæ¬¡çˆ¬å–ä½ç½®
        start_date = getattr(args, "start_date", None)
        if not start_date:
            last_time = crawler.get_last_fetch_time()
            if last_time:
                # ä»ä¸Šæ¬¡æœ€æ–°æ—¶é—´ç»§ç»­
                try:
                    dt = datetime.strptime(last_time, "%Y-%m-%dT%H:%M:%S.%f%z")
                    start_date = dt.strftime("%Y-%m-%d")
                    logger.info("æ˜Ÿçƒ %s å¢é‡çˆ¬å–ï¼Œä» %s å¼€å§‹", gid, last_time)
                except ValueError:
                    start_date = datetime.now().strftime("%Y-%m-%d")
            else:
                start_date = datetime.now().strftime("%Y-%m-%d")

        topics = await crawler.fetch_date_range(start_date, end_date)

        if topics:
            # æ ‡è®°æ¥æºæ˜Ÿçƒ
            for t in topics:
                t["group_id"] = gid
            crawler.update_last_fetch(topics)
            all_topics.extend(topics)
            logger.info("æ˜Ÿçƒ %s: è·å– %d æ¡å¸–å­", gid, len(topics))
        else:
            logger.info("æ˜Ÿçƒ %s: æš‚æ— æ–°å†…å®¹", gid)

    if not all_topics:
        notifier.send_text("ğŸ“­ æ‰€æœ‰æ˜Ÿçƒæš‚æ— æ–°å†…å®¹")
        return ""

    # ä¿å­˜åˆ°JSON
    data_dir = Path("data")
    data_dir.mkdir(parents=True, exist_ok=True)
    date_label = f"{start_date}_to_{end_date}" if len(group_ids) == 1 else end_date
    output_path = str(data_dir / f"topics_{date_label}.json")

    with open(output_path, "w", encoding="utf-8") as f:
        json.dump({"group_names": group_names, "topics": all_topics}, f, ensure_ascii=False, indent=2)

    logger.info("æ•°æ®å·²ä¿å­˜: %sï¼ˆ%d æ¡å¸–å­ï¼‰", output_path, len(all_topics))
    notifier.send_text(
        f"ğŸ“¥ æ•°æ®çˆ¬å–å®Œæˆ\n"
        f"ğŸ“Š æ˜Ÿçƒæ•°: {len(group_ids)}\n"
        f"ğŸ“ å¸–å­æ•°: {len(all_topics)}\n"
        f"ğŸ“„ æ–‡ä»¶: {output_path}"
    )
    return output_path


async def do_analyze(args) -> str:
    """è¯»å–JSONæ•°æ®ï¼Œè°ƒç”¨å¤§æ¨¡å‹åˆ†æï¼Œç”ŸæˆæŠ¥å‘Š"""
    logger = logging.getLogger(__name__)
    notifier = WeChatNotifier()

    data_path = args.data
    if not Path(data_path).exists():
        logger.error("æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: %s", data_path)
        return ""

    with open(data_path, "r", encoding="utf-8") as f:
        raw = json.load(f)

    # å…¼å®¹æ–°æ—§æ ¼å¼
    if isinstance(raw, dict) and "topics" in raw:
        topics = raw["topics"]
        group_names = raw.get("group_names", {})
    else:
        topics = raw
        group_names = {}

    logger.info("åŠ è½½äº† %d æ¡å¸–å­", len(topics))

    # AIåˆ†æ
    analyzer = SentimentAnalyzer(
        openai_api_key=get_config("openai_api_key"),
        anthropic_api_key=get_config("anthropic_api_key"),
    )
    analysis = await analyzer.analyze_topics(topics)

    if analysis.empty:
        notifier.send_text("ğŸ“­ å†…å®¹ä¸­æœªå‘ç°å¯åˆ†æçš„è´¢ç»ä¿¡æ¯")
        return ""

    # ä»æ–‡ä»¶åæå–æ—¥æœŸæ ‡ç­¾
    stem = Path(data_path).stem
    date_label = stem.replace("topics_", "") or datetime.now().strftime("%Y-%m-%d")

    # ç”ŸæˆæŠ¥å‘Š
    reporter = ReportGenerator()
    report_path = reporter.generate(analysis, topics, date=date_label, group_names=group_names)

    financial_count = len(analysis[analysis["is_financial"] == True]) if not analysis.empty else 0

    summary = (
        f"ğŸ“Š èˆ†æƒ…åˆ†æå®Œæˆ\n\n"
        f"ğŸ“ å¸–å­æ•°: {len(topics)}\n"
        f"ğŸ’° è´¢ç»ç›¸å…³: {financial_count} æ¡\n"
        f"ğŸ“„ æŠ¥å‘Š: {report_path}"
    )
    notifier.send_text(summary)
    notifier.send_file(report_path, "èˆ†æƒ…åˆ†ææŠ¥å‘Š")

    logger.info("=== åˆ†æå®Œæˆ ===")
    return report_path


async def do_run(args):
    """çˆ¬å–+åˆ†æä¸€æ¡é¾™"""
    logger = logging.getLogger(__name__)
    logger.info("=== çŸ¥è¯†æ˜Ÿçƒèˆ†æƒ…åˆ†æå¼€å§‹ ===")

    try:
        data_path = await do_fetch(args)
        if not data_path:
            return

        args.data = data_path
        await do_analyze(args)

        logger.info("=== å…¨éƒ¨å®Œæˆ ===")
    except Exception as e:
        logger.exception("è¿è¡Œå¼‚å¸¸")
        WeChatNotifier().send_alert(f"è¿è¡Œå¼‚å¸¸: {str(e)}")
        raise


def main():
    parser = argparse.ArgumentParser(description="çŸ¥è¯†æ˜Ÿçƒè‚¡ç¥¨èˆ†æƒ…åˆ†æå™¨")
    subparsers = parser.add_subparsers(dest="command", help="å­å‘½ä»¤")

    # fetch
    fetch_p = subparsers.add_parser("fetch", help="çˆ¬å–å¸–å­å’Œè¯„è®º")
    fetch_p.add_argument("--start-date", type=str, help="èµ·å§‹æ—¥æœŸ YYYY-MM-DDï¼ˆä¸ä¼ åˆ™å¢é‡ï¼‰")
    fetch_p.add_argument("--end-date", type=str, help="ç»“æŸæ—¥æœŸ YYYY-MM-DD")

    # analyze
    analyze_p = subparsers.add_parser("analyze", help="åˆ†æå·²çˆ¬å–çš„æ•°æ®")
    analyze_p.add_argument("--data", type=str, required=True, help="æ•°æ®JSONæ–‡ä»¶è·¯å¾„")

    # run
    run_p = subparsers.add_parser("run", help="çˆ¬å–+åˆ†æä¸€æ¡é¾™")
    run_p.add_argument("--start-date", type=str, help="èµ·å§‹æ—¥æœŸ YYYY-MM-DDï¼ˆä¸ä¼ åˆ™å¢é‡ï¼‰")
    run_p.add_argument("--end-date", type=str, help="ç»“æŸæ—¥æœŸ YYYY-MM-DD")

    args = parser.parse_args()
    setup_logging()

    if args.command == "fetch":
        asyncio.run(do_fetch(args))
    elif args.command == "analyze":
        asyncio.run(do_analyze(args))
    elif args.command == "run":
        asyncio.run(do_run(args))
    else:
        parser.print_help()


if __name__ == "__main__":
    main()
