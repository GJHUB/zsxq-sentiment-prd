"""çŸ¥è¯†æ˜Ÿçƒè‚¡ç¥¨èˆ†æƒ…åˆ†æå™¨ - ä¸»å…¥å£"""

import argparse
import asyncio
import logging
import sys
from datetime import datetime
from pathlib import Path

from src.auth import AuthManager
from src.config import CONFIG, get_config
from src.crawler import ZsxqCrawler
from src.analyzer import SentimentAnalyzer
from src.report import ReportGenerator
from src.notify import WeChatNotifier


def setup_logging():
    """é…ç½®æ—¥å¿—"""
    log_dir = get_config("log_dir", "logs")
    Path(log_dir).mkdir(parents=True, exist_ok=True)

    today = datetime.now().strftime("%Y-%m-%d")
    log_file = Path(log_dir) / f"{today}.log"

    logging.basicConfig(
        level=getattr(logging, get_config("log_level", "INFO")),
        format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
        handlers=[
            logging.FileHandler(log_file, encoding="utf-8"),
            logging.StreamHandler(sys.stdout),
        ],
    )


async def main():
    """ä¸»è¿è¡Œæµç¨‹"""
    parser = argparse.ArgumentParser(description="çŸ¥è¯†æ˜Ÿçƒè‚¡ç¥¨èˆ†æƒ…åˆ†æå™¨")
    parser.add_argument("--start-date", type=str, help="èµ·å§‹æ—¥æœŸ YYYY-MM-DD")
    parser.add_argument("--end-date", type=str, help="ç»“æŸæ—¥æœŸ YYYY-MM-DDï¼ˆé»˜è®¤ä»Šå¤©ï¼‰")
    args = parser.parse_args()

    setup_logging()
    logger = logging.getLogger(__name__)
    logger.info("=== çŸ¥è¯†æ˜Ÿçƒèˆ†æƒ…åˆ†æå¼€å§‹ ===")

    # 1. åˆå§‹åŒ–
    notifier = WeChatNotifier()
    auth = AuthManager(
        cookie_path=get_config("cookie_path"),
        notify_func=notifier.send_image,
    )

    try:
        # 2. è·å–æœ‰æ•ˆCookie
        cookie = await auth.get_cookie()
        if not cookie:
            notifier.send_alert("Cookieè·å–å¤±è´¥ï¼Œè¯·æ£€æŸ¥")
            logger.error("Cookieè·å–å¤±è´¥ï¼Œé€€å‡º")
            return

        # 3. çˆ¬å–æ•°æ®
        crawler = ZsxqCrawler(
            group_id=get_config("group_id"),
            cookie=cookie,
        )

        if args.start_date:
            # æ—¥æœŸèŒƒå›´æ¨¡å¼
            end_date = args.end_date or datetime.now().strftime("%Y-%m-%d")
            topics = await crawler.fetch_date_range(args.start_date, end_date)
            date_label = f"{args.start_date}_to_{end_date}"
        else:
            # é»˜è®¤ä»Šæ—¥æ¨¡å¼
            topics = await crawler.fetch_all_today()
            date_label = datetime.now().strftime("%Y-%m-%d")

        if not topics:
            notifier.send_text("ğŸ“­ æŒ‡å®šæ—¥æœŸèŒƒå›´å†…æš‚æ— æ–°å†…å®¹")
            logger.info("æŒ‡å®šæ—¥æœŸèŒƒå›´å†…æš‚æ— æ–°å†…å®¹")
            return

        logger.info("è·å–åˆ° %d æ¡å¸–å­", len(topics))

        # 4. AIåˆ†æ
        analyzer = SentimentAnalyzer(
            anthropic_api_key=get_config("anthropic_api_key"),
            openai_api_key=get_config("openai_api_key"),
        )
        analysis = await analyzer.analyze_topics(topics)

        if analysis.empty:
            notifier.send_text("ğŸ“­ ä»Šæ—¥å†…å®¹æœªæåŠå…·ä½“è‚¡ç¥¨")
            logger.info("ä»Šæ—¥å†…å®¹æœªæåŠå…·ä½“è‚¡ç¥¨")
            return

        # 5. ç”ŸæˆæŠ¥å‘Š
        reporter = ReportGenerator()
        report_path = reporter.generate(analysis, topics, date=date_label)

        # 6. ç»Ÿè®¡ä¿¡æ¯
        financial_count = len(analysis[analysis["is_financial"] == True]) if not analysis.empty else 0

        # 7. å‘é€ç»“æœ
        summary = (
            f"ğŸ“Š èˆ†æƒ…åˆ†æå®Œæˆ\n\n"
            f"ğŸ“… æ—¥æœŸ: {date_label}\n"
            f"ğŸ“ å¸–å­æ•°: {len(topics)}\n"
            f"ğŸ’° è´¢ç»ç›¸å…³: {financial_count} æ¡\n"
            f"ğŸ“„ æŠ¥å‘Š: {report_path}"
        )
        notifier.send_text(summary)
        notifier.send_file(report_path, "èˆ†æƒ…åˆ†ææŠ¥å‘Š")

        logger.info("=== åˆ†æå®Œæˆ ===")

    except Exception as e:
        logger.exception("è¿è¡Œå¼‚å¸¸")
        notifier.send_alert(f"è¿è¡Œå¼‚å¸¸: {str(e)}")
        raise


if __name__ == "__main__":
    asyncio.run(main())
